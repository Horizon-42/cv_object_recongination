# key fields

## Segment_anything

## Vision Transformers
### ViT (Dosovitskiy et al., 2020)

### DINO: Self-supervised learning for ViTs

## Self-Supervised and Unsupervised Learning
These methods reduce reliance on labeled data, making it feasible to train models on vast, unannotated datasets.
### SimCLR, MoCo, BYOL for self-supervised learning.

### DINO and CLIP for learning cross-modal embeddings.

## Multi-Modal Learning
Combines data from multiple sources

### CLIP (Contrastive Language-Image Pretraining)
### DALL-E and Stable Diffusion for image generation from text.

## 3D Vision and Object Recognition
Extends object recognition into 3D, enabling applications in robotics, AR/VR, and autonomous vehicles.

### PointNet, PointNet++, and Transformer-based 3D models.

### Neural Radiance Fields (NeRF) for 3D reconstruction.


##  Edge AI and Efficient Models
Brings advanced computer vision to resource-constrained devices such as smartphones, drones, and IoT devices.
### EfficientNet, MobileNet, and YOLOv7.
### Techniques like pruning, quantization, and knowledge distillation.

## Generative AI in Vision
Advances in generative models enable creating synthetic data for training and exploring creative AI applications.

### GANs (Generative Adversarial Networks) like StyleGAN3.

### Diffusion models such as Stable Diffusion and Imagen.

## Few-Shot and Zero-Shot Learning
Allows models to generalize to new tasks or categories with minimal or no additional training.

### ProtoNet and MetaOptNet for few-shot learning.
### CLIP and ALIGN for zero-shot recognition.
