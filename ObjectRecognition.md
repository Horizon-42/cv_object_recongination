# Advances in Deep Neural Networks for Vision: A Review

Deep neural networks(DNNs) play a critical role in computer vision. Frameworks like R-CNN, YOLO, and SSD have pushed boundaries in object detection, and models like U-Net, Mask R-CNN and DeepLab excel in tasks of semantic and instance segmentation. In this essay we will review the development of R-CNN family, while introducing a new framework, Vision Transformers(ViT).
R-CNN(Region-Based Convolutional Neural Network) is a family of object detection models designed to locate and classify objects within an image. The original R-CNN combined region proposals with CNNs, and introduced a auxiliary approach to fine-tune pre-trained models to tackle tasks with scarce labeled trining data. R-CNN use an external algorithm(e.g., Selective Search) to generate category-independent region proposals, and use a large CNN to extract a fixed-length feature vector from each region, and then use a set of linear SVMs to classify these features into object categories. At the end, R-CNN applies box regression to improve localization accuracy.
R-CNN is computationally expense due to separate stages for region proposal, feature extraction and classification. It also has slow inference speed, as the CNN processes each region proposal independently.
To overcome these drawbacks, Fast R-CNN proposed a RoI Pooling Layer, which converts variable-sized region proposals into fixed-size feature maps, enabling end-to-end training. The network first processes the whole image with several convolutional and max pooling layers to produce a conv feature map, then, use the RoI pooling layer to extract a fixed-length feature vector from the feature map. After that, Fast R-CNN uses a full connection layer to branch the feature vector into two sibling output layers: one produces softmax probability to estimate object category, and another layer that output the refined bounding-box.
Although Fast R-CNN has a significant performance advance compare to R-CNN, it still use Selective Search for region proposals, which is computational expensive and not learnable. This issue was addressed by Faster R-CNN, which introduced a Region Proposal Network(RPN). RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN network was merged with Fast R-CNN by sharing their convolutional features using attention mechanisms, telling the unified network where to look. With this architecture, Faster R-CNN truly performs end-to-end training, and has a much faster inference speed.
Mask R-CNN is an extension of R-CNN family, tailored for instance segmentation. It adds a branch to predict a segmentation mask for each detected object, making it suitable for pixel-level object segmentation. It builds on Faster R-CNN, adds a small FCN to predicts a binary mask for each object, and replaces RoIPool with RoIAlign to improve spatial alignment of features and mask predictions.
While the R-CNN family advancing and expanding, a new approach which using "attention" appeared, the Vision Transformer(ViT). It applies Transformer Architecture, which was originally developed for natural language processing, to the domain of computer vision. 
Instead of processing pixels locally using convolutional filters, it divided an image into fixed-size non-overlapping patches which is flattened and linearly embedded into a vector. These image patches are treated as tokens, passed through a Transformer encoder, which consists of multiple layers of self-attention and feed-forward networks, helping capture global dependencies and relationships between distant parts of the image. Position embeddings are added to the patch tokens to preserve information about the relative position of them in the image. The output of the encoder is used by a fully connected layer for image classification.
ViT can learn the global context of the input image, and scales well with the size of the dataset and model, outperforming CNN-based models when trained on large datasets. But it still has some limitation. It requires a large amount of training data to achieve high performance, and has high computational and memory requirements.


